---
author: "Tyler Pollard"
date: "2024-08-25"
header-includes:
  - \usepackage{mathtools}
output:  
  github_document:
    html_preview: false
    # includes: 
    #   in_header: _includes/head.html
    toc: true
    toc_depth: 3
---

```{r setup, echo = FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
```

```{r load libraries}
# Load Libraries ----
library(knitr)
library(data.table)
library(MASS)
library(bestNormalize)
library(plyr)
library(stringr)
library(lubridate)
library(sf)
library(spData)
library(tictoc)
library(cowplot)
library(GGally)
library(patchwork)
library(fitdistrplus)
library(caret)
library(splines)
library(mgcv)
library(DescTools)
library(car)
library(bayesplot)
library(BayesFactor)
library(rstanarm)
library(tidybayes)
library(loo)
library(brms)
library(bayesplot)
library(performance)
library(gt)
library(gtsummary)
library(tidyverse)
```

```{r Read in Data, warning=FALSE, message=FALSE}
# Read in data ----
Stormdata_raw <- fread("_data/E2_data.csv")
Actual_Y <- fread("_data/Actual Y.csv")
Actual_Yvec <- Actual_Y |> filter(complete.cases(x)) |> pull(x)

# Data ----
## Clean data ----
Stormdata1 <- Stormdata_raw |>
  mutate(
    Obs = 1:nrow(Stormdata_raw),
    StormID = factor(StormID),
    basin = factor(basin),
    Date = as_datetime(Date, tz = "UTC"),
    Year = year(Date),
    Month = month(Date, label = TRUE),
    Day = yday(Date),
    LON180 = LON - 360,
    DataType = ifelse(is.na(VMAX), "Test", "Train")
  ) |>
  group_by(StormID) |>
  mutate(
    StormElapsedTime = as.numeric(difftime(Date, min(Date), units = "hours")),
    StormElapsedTime2 = StormElapsedTime/6
  ) |>
  ungroup() |>
  select(
    DataType,
    StormID,
    Date,
    Year,
    Month,
    Day,
    basin,
    StormElapsedTime,
    StormElapsedTime2,
    LAT,
    LON,
    LON180,
    everything(),
    -lead_time
  ) |>
  mutate(
    LAT2 = LAT,
    .after = LAT
  ) |>
  mutate(
    LON2 = LON,
    .after = LON
  )

### Land Indicator ----
pts <- st_as_sf(Stormdata1, # |> select(LON, LAT), 
                coords = c("LON180", "LAT"),
                crs = 4326
)
land_pts <- !is.na(as.numeric(st_intersects(pts, world)))

Stormdata <- Stormdata1 |>
  mutate(
    Land = factor(land_pts, 
                  labels = c("Water", "Land"))
  ) |>
  select(
    DataType,
    StormID,
    Date,
    Year,
    Month,
    Day,
    basin,
    StormElapsedTime,
    StormElapsedTime2,
    LAT,
    LAT2,
    LON,
    LON2,
    LON180,
    Land,
    everything()
  ) |>
  mutate(
    across(c(StormID, Year, Month, basin),
           ~factor(.x)
    )
  )

## Split data -----
StormdataTrain <- Stormdata |>
  filter(DataType == "Train") |>
  select(-c(
    DataType,
    Date,
    Day,
    LON180,
    StormElapsedTime2,
    Obs
  ))|>
  mutate(
    across(c(StormID, Year, Month, basin),
           ~droplevels(.x)
    )
  )

StormdataTest <- Stormdata |>
  filter(DataType == "Test") |>
  select(-c(
    DataType,
    Date,
    Day,
    LON180,
    StormElapsedTime2,
    Obs
  ))|>
  mutate(
    across(c(StormID, Year, Month, basin),
           ~droplevels(.x)
    )
  ) |>
  mutate(
    VMAX = Actual_Yvec
  )

### Regular ----
preProc <- preProcess(StormdataTrain |> 
                        select(
                          where(is.numeric),
                          -VMAX
                          #-StormElapsedTime,
                          #-LAT,
                          #-LON
                        ),
                      method = c("scale", "center"))
#preProc
StormdataTrain2 <- predict(preProc, StormdataTrain)
StormdataTes2 <- predict(preProc, StormdataTest)
#colnames(StormdataTrain)

### YeoJohnson ----
preProcYeo <- preProcess(StormdataTrain |> 
                           select(
                             where(is.numeric),
                             -VMAX,
                             #-HWRF,
                             -StormElapsedTime,
                             -LAT2,
                             -LON2
                           ),
                         method = c("scale", "center", "YeoJohnson"))
#preProcYeo
StormdataTrainYeo <- predict(preProcYeo, StormdataTrain)
StormdataTestYeo <- predict(preProcYeo, StormdataTest)
#colnames(StormdataTrainYeo)


### Arcsinh ----
StormdataTrainArcsinhPre <- StormdataTrain |>
  mutate(
    across(c(where(is.numeric), 
             -VMAX, #-HWRF,
             -StormElapsedTime, 
             -LAT2, 
             -LON2),
           ~predict(arcsinh_x(.x, standardize = FALSE), newdata = .x)
    ))

StormdataTestArcsinhPre <- StormdataTest |>
  mutate(
    across(c(where(is.numeric), 
             -VMAX, #-HWRF,
             -StormElapsedTime, 
             -LAT2, 
             -LON2),
           ~predict(arcsinh_x(.x, standardize = FALSE), newdata = .x)
    ))

preProcArcsinh <- preProcess(StormdataTrainArcsinhPre |> 
                               select(
                                 where(is.numeric),
                                 -VMAX, #-HWRF,
                                 -StormElapsedTime,
                                 -LAT2,
                                 -LON2
                               ),
                             method = c("scale", "center"))
#preProcArcsinh
StormdataTrainArcsinh <- predict(preProcArcsinh, StormdataTrainArcsinhPre)
StormdataTestArcsinh <- predict(preProcArcsinh, StormdataTestArcsinhPre)
```

# Introduction

The data for this analysis are from [A Feed Forward Neural Network Based on Model Output Statistics for Short-Term Hurricane Intensity Prediction](https://journals.ametsoc.org/view/journals/wefo/34/4/waf-d-18-0173_1.xml), with detailed variable descriptions available [here](https://github.com/TylerPollard410/Hurricane-Analysis/blob/main/docs/data_description.pdf). This paper uses deep learning to improve 24-hour ahead forecasts of hurricane intensity (maximum wind velocity, VMAX). 

The primary prediction model, [HWRF](https://www.aoml.noaa.gov/hurricane-weather-research-forecast-model/), which is a mathematical weather prediction model based on differential equations. In addition to the forecast, HWRF has many other state variables such as **sea surface temperature, longitude, time of year, etc**, that are usually discarded. 

This analysis aims to assess whether incorporating these **HWRF state variables** can enhance the accuracy of hurricane intensity predictions

# Data Exploration (Shiny App)

To begin this analysis, I created a [Hurricane Analysis app](https://tylerpollard410.shinyapps.io/Hurricane_EDA/) to better understand and explore the data. The distribution of VMAX was plotted using the "Histogram" plot type, which revealed positive right skewness. The importance of spatial predictors like basin, Land, LAT, and LON can be viewed using the "Map" plot type. VMAX was plotted against all potential covariates and their transformed values to explore their relationship with the response and identify the best transformations to apply prior to model fitting.

To facilitate exploratory analysis, I developed an interactive [Hurricane Analysis app](https://tylerpollard410.shinyapps.io/Hurricane_EDA/). This tool enables the visualization of key variables and their relationships with VMAX:

* The Histogram plot reveals that **VMAX exhibits positive right-skewed distribution**.
* The Map feature illustrates the geographic influence of spatial variables like basin, land presence, latitude, and longitude.
* Scatter plots of VMAX against potential predictors highlight the importance of **feature transformations** for model fitting.

# Model Description

The response variable $Y_i$ is the observed VMAX of observation $i = 1, ..., n = 1705$ with StormID index $S_i = 1, ..., 87$ of a given hurricane. The $p = 20$ covariates $X_{ij}$ that were fit to the model were LAT, LON, basin, Land, MINSLP, SHR_MAG, STM_SPD, SST, RHL0, CAPE1, CAPE3, SHTFL2, TCOND7002, INST2, CP1, TCONDSYM2, COUPLSYM3, HWFI, VMAX_OP_T0, and HWRF. All covariates were normalized using a Yeo-Johnson transformation with centering and scaling. Due to the positive right skewness of VMAX, models were compared for Log-normal and Gamma likelihoods both with and without random effects.

## Lognormal

$$
\begin{aligned}
Y_{i} &\sim \text{LogNormal}(\mu_i, \sigma^2) \\
\mu_i &= \beta_{0} + \sum_{j=1}^{p}X_{ij}\beta_{j} + \theta_{S_i} \\
\theta_{S_i} &\sim \text{Normal}(0, \tau^2)
\end{aligned}
$$

## Gamma

$$
\begin{aligned}
Y_{i} &\sim \text{Gamma}(\alpha, \alpha/\mu_i) \\
log(\mu_i) &= \beta_{0} + \sum_{j=1}^{p}X_{ij}\beta_{j} + \theta_{S_i} \\
\theta_{S_i} &\sim \text{Normal}(0, \tau^2)
\end{aligned}
$$

where $\beta_{j}$ is the effect of covariate $j$ with weakly informative priors $\beta_j \sim \text{Normal}(0,5)$. We set a non-informative prior on $\alpha \sim \text{InvGamma}(0.1, 0.1)$. In the random effects models, we set non-informative priors $\tau \sim \text{InvGamma}(0.1, 0.1)$, otherwise $\theta_{S_i} = 0$.

# Model Comparisons

All four models above were fit to the data and were first checked that the models converged to an acceptable level. Once convergence was confirmed, the models were compared based on WAIC to determine the model with the best expected predictive accuracy. Based on Table 1, I chose to go forward with the Log-normal with random effects model because it had the smallest mean deviance which is a good sign that it will predict the best. Both Log-normal models performed better than their Gamma counterparts, which solidified it as the better of the two likelihoods for VMAX. Additionally, both random effect models performed better than the non-random effect models, providing evidence for a random intercept for StormID.

```{r WAIC}
load(file = "_data/waicComps.RData")

waicListComp |>
  select(
    Model, waic, p_waic, elpd_waic, elpd_diff, se_diff
  ) |>
  gt() |>
  # tab_header(
  #   title = "Table 1: Model Selection Criteria"
  # ) |>
  fmt_number(
    decimals = 2,
    use_seps = FALSE
  ) |>
  cols_align(
    align = "center",
    columns = -Model
  ) |>
  tab_style(
    style = list(
      cell_borders(sides = "right")
    ),
    locations = list(
      cells_body(columns = Model),
      cells_column_labels(columns = Model)
    )
  ) |>
  tab_options(
    table.align = "center"
  ) |>
  tab_style(
    style = list(
      cell_fill(color = "#373737"),
      cell_text(color = "#fff")
    ),
    locations = list(
      cells_column_labels()
    )
  ) |>
  tab_options(
    column_labels.padding = "10px",
    table.background.color = "#f2f2f2"
  ) |>
  #opt_vertical_padding(scale = 0.25) |>
  tab_caption(caption = md("Table 1: Model Selection Criteria")) |>
  as_raw_html()
```

## Final Model

The Log-normal with random effects model was further refined to obtain a final model. First, TCOND7002, INST2, and SHTFL2 were moved due to high VIF and the model was refit. Covariates were removed one at a time and the model was iteratively fit until the 95\% credible interval for all remaining covariates did not contain 0. The final model contained $p = 9$ covariates for Land, SHR_MAG, RHLO, CAPE3, CP1, TONDSYM2, COUPLSYM3, HWFI, and HWRF.

# Goodness of Fit

To verify goodness of fit of the model, draws were made from the posterior predictive distribution (PPD) and posterior predictive checks (PPC) were examined for the mean, standard deviation, and range. These checks can be seen below with corresponding Bayesian p-values. The p-values are far enough away from 0 or 1 to confirm the model adequately fits the data. The empirical distribution of the observed data also appears to fall within the simulated distributions from the PPD draws.

```{r GOF, fig.align='center', out.width='90%', fig.cap="Figure 1: Posterior Predictive Checks for Log-normal Random Effects Model"}
load(file = "_data/final_Fit.RData")

iters <- 4000
burn <- 2000
chains <- 4
sims <- (iters-burn)*chains

fillPPC <- "#d1e1ec"
colorPPC <- "#b3cde0"
fill2PPC <- "#011f4b"

## Posteriors ----
### Training ----
set.seed(52)
posteriorFit <- posterior_predict(final_Fit)
posteriorFitMean <- colMeans(posteriorFit)
posteriorFitMed <- apply(posteriorFit, 2, function(x){quantile(x, 0.5)})
posteriorFitLCB <- apply(posteriorFit, 2, function(x){quantile(x, 0.025)})
posteriorFitUCB <- apply(posteriorFit, 2, function(x){quantile(x, 0.975)})

## PPC ----
trainVMAX <- StormdataTrain$VMAX
testVMAX <- Actual_Yvec

set.seed(52)
sampFitID <- sample(1:sims, 1000, replace = FALSE)
postSampFit <- posteriorFit[sampFitID, ]

### Density -----
ppcDensPlot <- ppc_dens_overlay(
  y = trainVMAX,
  yrep = postSampFit
) +
  labs(
    # title = "Simulated density of draws from the PPD vs Observed VMAX",
    # subtitle = "n = 1000 draws",
    x = "VMAX",
    y = "Density"
  ) +
  scale_x_continuous(limits = c(0, 250), breaks = seq(0, 250, 25)) +
  theme_bw() +
  theme(
    legend.position = "none"
  )


#### Stats ----
# Make stat functions
meanFunc <- function(y){mean(y)}
sdFunc <- function(y){sd(y)}
rangeFunc <- function(y){max(y) - min(y)}

##### Mean ----
set.seed(52) # for reproducibility
meanY <- meanFunc(trainVMAX)

ppcMeanStat <- ppc_stat_data(
  y = trainVMAX,
  yrep = posteriorFit,
  group = NULL,
  stat = c("meanFunc")
) |>
  mutate(
    meanProbLow = value < meanY,
    meanProbHigh = value > meanY
  )

ppcMeanPlotGG <- ggplot() +
  geom_histogram(
    data = ppcMeanStat |> filter(variable != "y"),
    aes(x = value, color = "Posterior"),
    fill = fillPPC
  ) +
  geom_vline(
    data = ppcMeanStat |> filter(variable == "y"),
    aes(xintercept = value, color = "Observed"),
    linewidth = 1
  ) +
  scale_x_continuous(
    name = "VMAX"
  ) +
  scale_y_continuous(
    name = "Number of Posterior Draws",
    expand = expansion(mult = c(0, 0.01))
  ) +
  scale_color_manual(
    name = "Data",
    values = c(
      "Posterior" = colorPPC,
      "Observed" = "black"
    ),
    breaks = c("Posterior", "Observed")
  ) +
  labs(title = "Mean",
       subtitle = paste("p-value =", round(mean(ppcMeanStat$meanProbLow[-1]), 4))
  ) +
  theme_bw() +
  guides(color = guide_legend(byrow = TRUE)) +
  theme(
    panel.grid.major.x = element_blank(),
    panel.grid.minor.x = element_blank(),
    legend.key.spacing.y = unit(5, "points"),
    legend.position = "bottom",
    legend.direction = "horizontal"
  )

##### SD ----
set.seed(52) # for reproducibility
sdY <- sdFunc(trainVMAX)

ppcSDStat <- ppc_stat_data(
  y = trainVMAX,
  yrep = posteriorFit,
  group = NULL,
  stat = c("sdFunc")
) |>
  mutate(
    sdProbLow = value < sdY,
    sdProbHigh = value > sdY
  )

ppcSDPlotGG <- ggplot() +
  geom_histogram(
    data = ppcSDStat |> filter(variable != "y"),
    aes(x = value, color = "Posterior"),
    fill = fillPPC
  ) +
  geom_vline(
    data = ppcSDStat |> filter(variable == "y"),
    aes(xintercept = value, color = "Observed"),
    linewidth = 1
  ) +
  scale_x_continuous(
    name = "VMAX"
  ) +
  scale_y_continuous(
    name = "Number of Posterior Draws",
    expand = expansion(mult = c(0, 0.01))
  ) +
  scale_color_manual(
    name = "Data",
    values = c(
      "Posterior" = colorPPC,
      "Observed" = "black"
    ),
    breaks = c("Posterior", "Observed")
  ) +
  labs(title = "SD",
       subtitle = paste("p-value =", round(mean(ppcSDStat$sdProbLow[-1]), 4))
  ) +
  theme_bw() +
  guides(color = guide_legend(byrow = TRUE)) +
  theme(
    panel.grid.major.x = element_blank(),
    panel.grid.minor.x = element_blank(),
    legend.key.spacing.y = unit(5, "points"),
    legend.position = "bottom",
    legend.direction = "horizontal"
  )

##### Range ----
set.seed(52) # for reproducibility
rangeY <- rangeFunc(trainVMAX)

ppcRangeStat <- ppc_stat_data(
  y = trainVMAX,
  yrep = posteriorFit,
  group = NULL,
  stat = c("rangeFunc")
) |>
  mutate(
    rangeProbLow = value < rangeY,
    rangeProbHigh = value > rangeY
  )

ppcRangePlotGG <- ggplot() +
  geom_histogram(
    data = ppcRangeStat |> filter(variable != "y"),
    aes(x = value, color = "Posterior"),
    fill = fillPPC
  ) +
  geom_vline(
    data = ppcRangeStat |> filter(variable == "y"),
    aes(xintercept = value, color = "Observed"),
    linewidth = 1
  ) +
  scale_x_continuous(
    name = "VMAX"
  ) +
  scale_y_continuous(
    name = "Number of Posterior Draws",
    expand = expansion(mult = c(0, 0.01))
  ) +
  scale_color_manual(
    name = "Data",
    values = c(
      "Posterior" = colorPPC,
      "Observed" = "black"
    ),
    breaks = c("Posterior", "Observed")
  ) +
  labs(title = "Range",
       subtitle = paste("p-value =", round(mean(ppcRangeStat$rangeProbLow[-1]), 4))
  ) +
  theme_bw() +
  guides(color = guide_legend(byrow = TRUE)) +
  theme(
    panel.grid.major.x = element_blank(),
    panel.grid.minor.x = element_blank(),
    legend.key.spacing.y = unit(5, "points"),
    legend.position = "bottom",
    legend.direction = "horizontal"
  )


#### Combined plot ----
ppcCombPlot <- 
  ppcDensPlot /
  (ppcMeanPlotGG + ppcSDPlotGG + ppcRangePlotGG) +
  plot_layout(
    guides = "collect",
    axes = "collect_x"
  ) +
  plot_annotation(
    #title = "Posterior Predictive Checks for Distributional Statistics",
    #subtitle = paste("Bayesian predictive p-values for", sims, "Simulations"),
    theme = theme(
      legend.position = "bottom",
      legend.direction = "horizontal"
    )
  )
ppcCombPlot
```

# Variable Importance

After fitting the final model with random intercepts and weakly informative priors, the importance of the covariates was examined. The mean, standard deviation, and 95% credible set on each parameter is displayed for the posterior after partially pooling over the storm IDs. Since all covariates were center and scaled, the magnitude of the posterior means allow for direct comparison of covariate importance. From Table 2 below, the most important covariates from the model on VMAX were Land, HWFI, and HWRF. From the problem description, it is not suprising that HWRF was one of the most important covariates in modeling VMAX due to it being a prior model for VMAX already. However, it appears that HWFI actually is more important in this model for VMAX compared to HWRF.

```{r Var}
finalFitSum <- summary(final_Fit, digits = 4)
finalFitFixed <- finalFitSum$fixed
finalFitRand <- finalFitSum$random$StormID

finalFitFixed |>
  select(
    Mean = Estimate, 
    SD = Est.Error, 
    `Q2.5` = `l-95% CI`,
    `Q97.5` = `u-95% CI`
  ) |>
  round(digits = 4) |>
  rownames_to_column(var = "Parameter") |>
  gt() |>
  # tab_header(
  #   title = "Table 2: Posterior summary for model parameters"
  # ) |>
  cols_align(
    align = "center",
    columns = c(everything(), -Parameter)
  ) |>
  tab_style(
    style = list(
      cell_borders(sides = "right")
    ),
    locations = list(
      cells_body(columns = Parameter),
      cells_column_labels(columns = Parameter)
    )
  ) |>
  tab_style(
    style = list(
      cell_fill(color = "#373737"),
      cell_text(color = "#fff")
    ),
    locations = list(
      cells_column_labels()
    )
  ) |>
  tab_options(
    column_labels.padding = "10px",
    table.background.color = "#f2f2f2"
  ) |>
  #opt_vertical_padding(scale = 0.25) |>
  tab_caption(caption = md("Table 2: Posterior summary for model parameters")) |>
  as_raw_html()
```

# Prediction

In the data that was delivered for this project, there were an additional 668 observations for new StormIDs with missing VMAX. The predictive performance of our model will be evaluated based on the computed posterior predictive mean and 95\% credible interval for these out-of-sample data.

## Cross-Validation

```{r CV Prediction}
load(file = "_data/final_cv.RData")
load(file = "_data/final_cv_Metrics.RData")

trainHWRF <- StormdataTrain$HWRF

### Model ----
final_Fit_predMetrics <- tibble(
  Method = "Model PPD",
  MAE_HWRF = mean(abs(trainHWRF - trainVMAX)),
  MAE_Model = mean(abs(posteriorFitMean - trainVMAX)),
  COV = mean(posteriorFitLCB < trainVMAX & trainVMAX < posteriorFitUCB)
)

### CV ----
predMetricDF <- bind_rows(
  final_Fit_predMetrics,
  final_cv_Metrics |> 
    select(
      MAE_HWRF,
      MAE_Model = MAE_kfold,
      COV = COV_kfold
    ) |>
    round(digits = 3) |>
    mutate(
      Method = "5-fold CV",
      .before = 1
    )
)
```

To estimate the predictive accuracy of these model predictions, 5-fold cross-validation (CV) was used to estimate the mean absolute error (MAE) (i.e., the average of $|VMAX_i - \widehat{VMAX_i}|$ for the $i$ observations) and coverage (COV) of 95\% intervals. The 1705 observations were split into 5 approximately equal folds split by randomly selected StormIDs to mimic the prediction of entirely new hurricanes. Along with the predictions from CV, predictions were made using the PPD from the model treating the fit observations as missing for comparison. The prediction metrics from both methods are Table 3 below. The CV COV of `r round(final_cv_Metrics$COV_kfold, 3)` provides strong evidence that the model predicts new data well as it captured almost exactly the identified proportion of data expected from a 95\% credible interval. The MAE of `r round(final_cv_Metrics$MAE_kfold, 3)` from CV, which is about a `r round(1 - final_cv_Metrics$MAE_kfold/final_cv_Metrics$MAE_HWRF, 2)*100`\% improvement in prediction accuracy, further solidifies the preference for the Log-normal Random Effects model over the HWRF model alone. 

```{r CV Prediction Comp}
predMetricDF |>
  gt() |>
  # tab_header(
  #   title = "Table 3: Prediction metrics"
  # ) |>
  fmt_number(
    decimals = 3
  ) |>
  cols_label(
    MAE_HWRF = "HWRF MAE",
    MAE_Model = "Model MAE"
  ) |>
  cols_align(
    align = "center",
    columns = -Method
  ) |>
  tab_style(
    style = list(
      cell_borders(sides = "right")
    ),
    locations = list(
      cells_body(columns = Method),
      cells_column_labels(columns = Method)
    )
  ) |>
  tab_style(
    style = list(
      cell_fill(color = "#373737"),
      cell_text(color = "#fff")
    ),
    locations = list(
      cells_column_labels()
    )
  ) |>
  tab_options(
    column_labels.padding = "10px",
    table.background.color = "#f2f2f2"
  ) |>
  #opt_vertical_padding(scale = 0.25) |>
  tab_caption(caption = md("Table 3: Cross-Validation prediction metrics on observed data")) |>
  as_raw_html()
```

## Out-of-Sample 

```{r OOS Prediction}
### Posterior ----
set.seed(52)
posteriorPred <- posterior_predict(final_Fit,
                                   newdata = StormdataTestArcsinh,
                                   allow_new_levels = TRUE, 
                                   re_formula = NULL)
posteriorPredMean <- colMeans(posteriorPred)
posteriorPredMed <- apply(posteriorPred, 2, function(x){quantile(x, 0.5)})
posteriorPredLCB <- apply(posteriorPred, 2, function(x){quantile(x, 0.025)})
posteriorPredUCB <- apply(posteriorPred, 2, function(x){quantile(x, 0.975)})

#### Prediction Metrics ----
trainVMAX <- StormdataTrain$VMAX
testVMAX <- Actual_Yvec

trainHWRF <- StormdataTrain$HWRF
testHWRF <- StormdataTest$HWRF

OOS_predMetrics <- tibble(
  Method = "Model PPD",
  MAE_HWRF = mean(abs(testHWRF - testVMAX)),
  MAE_Model = mean(abs(posteriorPredMean - testVMAX)),
  MAD_Model = mean(abs(posteriorPredMed - testVMAX)),
  COV = mean(posteriorPredLCB < testVMAX & testVMAX < posteriorPredUCB)
)
```

Following this project, I was able to obtain the actual VMAX values of the 668 out-of-sample observations. Using this new data, I was able to quantify the true performance of our model.

```{r OOS Performance Table}
OOS_predMetrics |>
  gt() |>
  # tab_header(
  #   title = "Table 3: Prediction metrics"
  # ) |>
  fmt_number(
    decimals = 3
  ) |>
  cols_label(
    MAE_HWRF = "HWRF MAE",
    MAE_Model = "Model MAE",
    MAD_Model = "Model MAD"
  ) |>
  cols_align(
    align = "center",
    columns = -Method
  ) |>
  tab_style(
    style = list(
      cell_borders(sides = "right")
    ),
    locations = list(
      cells_body(columns = Method),
      cells_column_labels(columns = Method)
    )
  ) |>
  tab_style(
    style = list(
      cell_fill(color = "#373737"),
      cell_text(color = "#fff")
    ),
    locations = list(
      cells_column_labels()
    )
  ) |>
  tab_options(
    column_labels.padding = "10px",
    table.background.color = "#f2f2f2"
  ) |>
  #opt_vertical_padding(scale = 0.25) |>
  tab_caption(caption = md("Table 4: Prediction metrics on out-of-sample data")) |>
  as_raw_html()
```

Below is a plot of the out-of-sample PPD mean with 95\% CI for all of the new StormIDs compared to the actual VMAX values.  

```{r OOS All Plot, fig.align='center', out.width='90%', out.height='100%', fig.cap="Figure 2: "}
oosPosteriorDF <- bind_cols(
  StormdataTest,
  LCB = posteriorPredLCB,
  Mean = posteriorPredMean,
  Med = posteriorPredMed,
  UCB = posteriorPredUCB
) |>
  mutate(
    VMAX = Actual_Yvec
  ) 

fillPPD <- "lightblue"
colorActual <- "#011f4b"
colorHWRF <- "coral"
colorPPD <- "springgreen3"

#### All ----
oosPPD_plot_All <- ggplot(data = oosPosteriorDF, aes(x = StormElapsedTime)) +
  geom_ribbon(aes(ymin = LCB, ymax = UCB, fill = "95% CI"), alpha = 0.5) +
  geom_line(aes(y = VMAX, color = "Actual"), linewidth = 1, alpha = 0.5) +
  #geom_line(aes(y = HWRF, color = "HWRF")) +
  geom_line(aes(y = Mean, color = "PPD Mean"), linewidth = 0.75) +
  scale_y_continuous(limits = c(0,250), breaks = seq(0,275,50)) +
  facet_wrap(vars(StormID))+#, ncol = 6)+
  labs(
    #title = "PPD Mean vs Observed VMAX",
    #subtitle = "95% Credible Interval about PPD Mean",
    x = "Storm Elapsed Time"
  ) +
  scale_color_manual(name = NULL, 
                     breaks = c(
                       "Actual",
                       #"HWRF",
                       "PPD Mean"
                     ),
                     values = c(
                       "Actual" = colorActual,
                       #"HWRF" = colorHWRF,
                       "PPD Mean" = colorPPD
                     )
  ) +
  scale_fill_manual(name = NULL, 
                    values = c(
                      "95% CI" = fillPPD
                    )
  ) +
  guides(
    color = guide_legend(override.aes = list(linewidth = 1), order = 2)
  ) +
  theme_bw() +
  theme(
    legend.position = "bottom"
  )
oosPPD_plot_All
```

Below is a plot of the out-of-sample PPD mean with 95\% CI for new StormIDs that lasted the longest compared to the actual VMAX and HWRF values.

```{r OOS Long Plot, fig.align='center', out.width='90%', out.height='100%', fig.cap="Figure 3: "}
#### Long 9 ----
predStorms <- oosPosteriorDF |> 
  group_by(StormID) |>
  summarise(
    MaxTime = max(StormElapsedTime)
  ) |>
  #arrange(desc(MaxTime))
  arrange(MaxTime)

predStormsLong <- tail(predStorms, 9)

predStormsLongID <- predStormsLong |> 
  arrange(MaxTime) |>
  mutate(
    StormID = factor(as.character(StormID),
                     levels = as.character(StormID))
  )
#predStormsLongID

oosPosteriorDF_Long <- left_join(
  predStormsLongID,
  oosPosteriorDF
)

oosPPD_plot_Long <- 
  ggplot(data = oosPosteriorDF_Long, 
         aes(x = StormElapsedTime)) +
  geom_ribbon(aes(ymin = LCB, ymax = UCB, fill = "95% CI"), alpha = 0.5) +
  geom_line(aes(y = VMAX, color = "Actual"), linewidth = 1, alpha = .5) +
  geom_line(aes(y = HWRF, color = "HWRF"), linewidth = .75) +
  geom_line(aes(y = Mean, color = "PPD Mean"), linewidth = .75) +
  scale_y_continuous(limits = c(0,250), breaks = seq(0,275,50)) +
  facet_wrap(vars(StormID), dir = "h")+#, ncol = 6)+
  labs(
    #title = "PPD Mean vs Observed VMAX",
    #subtitle = "95% Credible Interval about PPD Mean",
    x = "Storm Elapsed Time"
  ) +
  scale_color_manual(name = NULL, 
                     breaks = c(
                       "Actual",
                       "HWRF",
                       "PPD Mean"
                       #"HWRF" = "dodgerblue"
                     ), 
                     values = c(
                       "Actual" = colorActual,
                       "HWRF" = colorHWRF,
                       "PPD Mean" = colorPPD
                     )
  )  +
  scale_fill_manual(name = NULL, 
                    values = c(
                      "95% CI" = fillPPD
                    )
  ) +
  guides(
    color = guide_legend(override.aes = list(linewidth = 1), order = 2)
  ) +
  theme_bw() +
  theme(
    legend.position = "bottom"
  )
oosPPD_plot_Long
```